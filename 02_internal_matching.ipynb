{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hieuxuan1112/bilingual-document-retrieval/blob/main/02_internal_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup & Dependencies"
      ],
      "metadata": {
        "id": "jO_vYnHwU6HK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UjK3Ej8zSKYC"
      },
      "outputs": [],
      "source": [
        "!pip install -q pymupdf sentence-transformers google-generativeai pdfplumber python-docx\n",
        "!pip install -q scikit-learn beautifulsoup4 requests nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuration & Model Initialization"
      ],
      "metadata": {
        "id": "sPnkMAd6VFgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import requests\n",
        "import fitz  # pymupdf\n",
        "import io\n",
        "import os\n",
        "from PIL import Image\n",
        "import docx\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from google.colab import userdata # <--- Quan trọng: Thư viện để đọc Secrets\n",
        "\n",
        "# Tải gói dữ liệu cần thiết cho nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "# --- CẤU HÌNH (LẤY TỪ SECRETS) ---\n",
        "try:\n",
        "    API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    print(\"✅ Đã lấy API Key thành công từ Secrets.\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ Lỗi: Không tìm thấy thư viện google.colab (Chạy trên local?).\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ CHƯA CẤU HÌNH SECRETS! Hãy thêm 'GEMINI_API_KEY' vào biểu tượng Chìa khóa bên trái.\")\n",
        "\n",
        "model_gemini = genai.GenerativeModel('gemini-flash-latest')\n",
        "\n",
        "# Load model so sánh câu\n",
        "labse_model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
        "\n",
        "print(\"Đã cấu hình xong Model: gemini-flash-latest\")"
      ],
      "metadata": {
        "id": "T42MnKPgSdpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8646bdd1-d5a8-480c-b825-6019e9ac843d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Đã lấy API Key thành công từ Secrets.\n",
            "Đã cấu hình xong Model: gemini-flash-latest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Gemini Helper Functions (Content Analysis)"
      ],
      "metadata": {
        "id": "IwqHqq7zVVsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_info_from_gemini(input_data, data_type):\n",
        "    \"\"\"\n",
        "    Hàm cải tiến (V2): Lấy cả TỪ KHÓA (để search) và NỘI DUNG (để so khớp LaBSE).\n",
        "    Trả về: (keywords_list, content_text_for_labse)\n",
        "    \"\"\"\n",
        "    if not input_data: return [], \"\"\n",
        "\n",
        "    # Prompt mới: Yêu cầu Gemini vừa OCR/Dịch vừa sinh từ khóa và định dạng rõ ràng\n",
        "    base_prompt = \"\"\"\n",
        "    Bạn là chuyên gia Hán Nôm và Sử học. Hãy phân tích hình ảnh hoặc văn bản đầu vào.\n",
        "\n",
        "    Yêu cầu 1 (Nội dung để so khớp):\n",
        "    - Nếu là Ảnh chữ Hán/Nôm: Hãy OCR nhận dạng các chữ trong ảnh rồi dịch nghĩa vắn tắt sang Tiếng Việt.\n",
        "    - Nếu là Ảnh Tiếng Việt/Văn bản: Hãy tóm tắt nội dung chính.\n",
        "    -> Viết kết quả thành một đoạn văn xuôi tiếng Việt (khoảng 100-200 chữ).\n",
        "\n",
        "    Yêu cầu 2 (Từ khóa tìm kiếm):\n",
        "    - Đề xuất 5 từ khóa tìm kiếm (search queries) hiệu quả nhất để tìm tài liệu này trên Google.\n",
        "\n",
        "    BẮT BUỘC TRẢ VỀ THEO ĐỊNH DẠNG SAU:\n",
        "    ---CONTENT_START---\n",
        "    (Viết đoạn văn nội dung ở đây)\n",
        "    ---CONTENT_END---\n",
        "    (Viết các từ khóa ở dưới đây, mỗi từ một dòng)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        if data_type == 'image':\n",
        "            # Gửi ảnh + Prompt\n",
        "            response = model_gemini.generate_content([base_prompt, input_data])\n",
        "        else:\n",
        "            # Gửi Text + Prompt\n",
        "            text_prompt = base_prompt + f\"\\n\\nNội dung đầu vào:\\n{input_data[:2000]}\"\n",
        "            response = model_gemini.generate_content(text_prompt)\n",
        "\n",
        "        # Xử lý text trả về để tách Nội dung và Từ khóa\n",
        "        full_response = response.text.strip()\n",
        "\n",
        "        # 1. Tách nội dung giữa 2 thẻ marker\n",
        "        extracted_content = \"\"\n",
        "        keywords = []\n",
        "\n",
        "        content_match = re.search(r'---CONTENT_START---(.*?)---CONTENT_END---', full_response, re.DOTALL)\n",
        "        if content_match:\n",
        "            extracted_content = content_match.group(1).strip()\n",
        "\n",
        "            # 2. Lấy phần còn lại làm keywords\n",
        "            keywords_raw = full_response.split('---CONTENT_END---')[-1].strip()\n",
        "            keywords = [k.strip().strip('-').strip() for k in keywords_raw.split('\\n') if k.strip()]\n",
        "        else:\n",
        "            # Dự phòng nếu Gemini quên format: Lấy dòng đầu làm content, các dòng sau làm keyword\n",
        "            lines = full_response.split('\\n')\n",
        "            extracted_content = lines[0]\n",
        "            keywords = lines[1:]\n",
        "\n",
        "        return keywords, extracted_content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Lỗi Gemini ({data_type}): {e}\")\n",
        "        return [], \"\""
      ],
      "metadata": {
        "id": "uMUZiOFBShNO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Dataset Preparation"
      ],
      "metadata": {
        "id": "vrcLug1XVeVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Thêm tham số -o (overwrite) để tự động ghi đè file cũ\n",
        "!unzip -qo Han_chapters.zip -d dataset_han\n",
        "# Lưu ý: Nhìn ảnh của bạn thì tên file là Viet_chapers.zip (thiếu chữ t),\n",
        "# nếu lệnh dưới lỗi thì bạn sửa thành Viet_chapers.zip nhé\n",
        "!unzip -qo Viet_chapters.zip -d dataset_viet\n",
        "\n",
        "print(\"Đã giải nén xong!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuVecIOuGFkz",
        "outputId": "07608e3a-570c-4853-8a12-374ba736fb1c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đã giải nén xong!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Online Search Utilities (Mode 2 - WIP)"
      ],
      "metadata": {
        "id": "lvk7JOBNVqUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_vietnamese_docs(sentences, max_results=5):\n",
        "    # Lấy key SerpApi từ két sắt\n",
        "    from google.colab import userdata\n",
        "    try:\n",
        "        serp_key = userdata.get('SERPAPI_KEY')\n",
        "    except Exception:\n",
        "        serp_key = \"\"\n",
        "        print(\"⚠️ CẢNH BÁO: Chưa cấu hình 'SERPAPI_KEY' trong Secrets. Chức năng tìm kiếm online sẽ lỗi!\")\n",
        "\n",
        "    results = []\n",
        "    for s in sentences:\n",
        "        params = {\n",
        "            \"engine\": \"google\",\n",
        "            \"q\": s[:80] + \" filetype:pdf\",\n",
        "            \"hl\": \"vi\",\n",
        "            \"gl\": \"vn\",\n",
        "            \"api_key\": serp_key  # <--- DÙNG BIẾN serp_key (AN TOÀN)\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            r = requests.get(\"https://serpapi.com/search\", params=params)\n",
        "            data = r.json()\n",
        "\n",
        "            if \"organic_results\" in data:\n",
        "                for res in data[\"organic_results\"]:\n",
        "                    if \"link\" in res:\n",
        "                        results.append(res[\"link\"])\n",
        "            elif \"error\" in data:\n",
        "                print(f\"⚠️ Lỗi từ SerpApi: {data['error']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Lỗi kết nối SerpApi: {e}\")\n",
        "\n",
        "    return list(set(results))[:max_results]"
      ],
      "metadata": {
        "id": "U7uajmvBSsO6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_files(urls):\n",
        "    paths = []\n",
        "    for i, url in enumerate(urls):\n",
        "        try:\n",
        "            r = requests.get(url, timeout=15)\n",
        "            if r.status_code == 200 and len(r.content) > 50000:\n",
        "                path = f\"vi_doc_{i}.pdf\"\n",
        "                with open(path, \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "                paths.append(path)\n",
        "        except:\n",
        "            pass\n",
        "    return paths\n"
      ],
      "metadata": {
        "id": "zRmDM3GfSu7L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Core Logic (Hybrid Matching V5) & Main Interface"
      ],
      "metadata": {
        "id": "_tNnxty5V8xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import requests\n",
        "import fitz # PyMuPDF\n",
        "from google.colab import files\n",
        "from sentence_transformers import util\n",
        "from PIL import Image\n",
        "import re\n",
        "\n",
        "# Thiết lập đường dẫn dữ liệu\n",
        "HAN_FOLDER = \"dataset_han\"\n",
        "VIET_FOLDER = \"dataset_viet\"\n",
        "\n",
        "# ==============================================================================\n",
        "# HÀM CHUYỂN ĐỔI TÊN FILE HÁN -> VIỆT (CHỈ ĐỂ HIỂN THỊ)\n",
        "# ==============================================================================\n",
        "def dich_ten_file_han(ten_file):\n",
        "    tu_vung = {\n",
        "        \"外紀\": \"Ngoại Kỷ\", \"本紀\": \"Bản Kỷ\", \"續編\": \"Tục Biên\",\n",
        "        \"卷\": \" Quyển \", \"之\": \"\", \".txt\": \"\"\n",
        "    }\n",
        "    for han, viet in tu_vung.items():\n",
        "        ten_file = ten_file.replace(han, viet)\n",
        "\n",
        "    so_dem = {\n",
        "        \"二十\": \"20\", \"十九\": \"19\", \"十八\": \"18\", \"十七\": \"17\", \"十六\": \"16\",\n",
        "        \"十五\": \"15\", \"十四\": \"14\", \"十三\": \"13\", \"十二\": \"12\", \"十一\": \"11\",\n",
        "        \"十\": \"10\", \"九\": \"9\", \"八\": \"8\", \"七\": \"7\", \"六\": \"6\",\n",
        "        \"五\": \"5\", \"四\": \"4\", \"三\": \"3\", \"二\": \"2\", \"一\": \"1\"\n",
        "    }\n",
        "    for han, viet in so_dem.items():\n",
        "        ten_file = ten_file.replace(han, viet)\n",
        "\n",
        "    return ten_file.strip()\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. HÀM GỌI GEMINI:\n",
        "#    - Trích xuất từ khóa chính\n",
        "#    - Dịch mẫu đoạn đầu để làm truy vấn ngữ nghĩa\n",
        "# ==============================================================================\n",
        "def analyze_content_with_gemini(han_text):\n",
        "    \"\"\"\n",
        "    Sử dụng Gemini để:\n",
        "    (1) Lấy danh sách thực thể / danh từ riêng quan trọng\n",
        "    (2) Dịch thử một đoạn ngắn ở đầu văn bản sang tiếng Việt\n",
        "    \"\"\"\n",
        "    chunk = han_text[:2500]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Bạn là trợ lý AI hỗ trợ tra cứu sử liệu. Hãy xử lý văn bản Hán Nôm sau:\n",
        "\n",
        "    Nhiệm vụ 1: Liệt kê 10-15 DANH TỪ RIÊNG quan trọng nhất (Nhân vật, Địa danh, Niên hiệu) bằng tiếng Việt.\n",
        "    Nhiệm vụ 2: Dịch một đoạn văn khoảng 200 chữ ở phần đầu văn bản sang tiếng Việt để làm mẫu so sánh.\n",
        "\n",
        "    Văn bản Hán:\n",
        "    {chunk}\n",
        "\n",
        "    Trả về đúng định dạng JSON, không thêm chú thích:\n",
        "    {{\n",
        "        \"keywords\": [\"Từ 1\", \"Từ 2\", ...],\n",
        "        \"translation\": \"Nội dung dịch đoạn văn mẫu...\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model_gemini.generate_content(prompt)\n",
        "        text_resp = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        import json\n",
        "        data = json.loads(text_resp)\n",
        "        return data.get(\"keywords\", []), data.get(\"translation\", \"\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Lỗi] Không thể xử lý phản hồi từ Gemini: {e}\")\n",
        "        return [], \"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. TÌM KIẾM HAI GIAI ĐOẠN:\n",
        "#    - Giai đoạn 1: Lọc nhanh bằng từ khóa\n",
        "#    - Giai đoạn 2: Xếp hạng lại bằng độ tương đồng ngữ nghĩa (LaBSE)\n",
        "# ==============================================================================\n",
        "def find_best_match_v5(source_file_path, target_folder):\n",
        "    # Bước 1: Đọc nội dung văn bản Hán\n",
        "    with open(source_file_path, 'r', encoding='utf-8') as f:\n",
        "        han_content = f.read()\n",
        "\n",
        "    print(\"Đang phân tích nội dung văn bản nguồn bằng Gemini...\")\n",
        "    keywords, translated_chunk = analyze_content_with_gemini(han_content)\n",
        "\n",
        "    if not keywords:\n",
        "        print(\"Không lấy được từ khóa từ Gemini.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Số từ khóa trích xuất được: {len(keywords)}\")\n",
        "    print(f\"Đoạn dịch mẫu dùng để so khớp ngữ nghĩa (rút gọn): {translated_chunk[:80]}...\")\n",
        "\n",
        "    # Bước 2: Lọc sơ bộ các file tiếng Việt bằng từ khóa\n",
        "    print(\"\\nGiai đoạn 1: Lọc sơ bộ theo từ khóa\")\n",
        "    files = glob.glob(os.path.join(target_folder, \"**/*.txt\"), recursive=True)\n",
        "    candidates = []\n",
        "\n",
        "    for f_path in files:\n",
        "        try:\n",
        "            with open(f_path, 'r', encoding='utf-8') as f:\n",
        "                tgt_content = f.read().lower()\n",
        "\n",
        "            match_count = 0\n",
        "            for kw in keywords:\n",
        "                if kw.lower() in tgt_content:\n",
        "                    match_count += 1\n",
        "\n",
        "            if match_count > 0:\n",
        "                score_kw = match_count / len(keywords)\n",
        "                candidates.append((score_kw, f_path))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "    top_candidates = candidates[:5]\n",
        "\n",
        "    if not top_candidates:\n",
        "        print(\"Không có file nào vượt qua vòng lọc từ khóa.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Chọn {len(top_candidates)} ứng viên vào vòng đánh giá ngữ nghĩa:\")\n",
        "    for s, f in top_candidates:\n",
        "        print(f\" - {os.path.basename(f)} | Mức khớp từ khóa: {s*100:.0f}%\")\n",
        "\n",
        "    # Bước 3: So khớp ngữ nghĩa bằng LaBSE\n",
        "    print(\"\\nGiai đoạn 2: So khớp ngữ nghĩa bằng embedding (LaBSE)\")\n",
        "\n",
        "    query_emb = labse_model.encode(translated_chunk, convert_to_tensor=True)\n",
        "    final_results = []\n",
        "\n",
        "    for kw_score, f_path in top_candidates:\n",
        "        try:\n",
        "            with open(f_path, 'r', encoding='utf-8') as f:\n",
        "                tgt_content = f.read()\n",
        "\n",
        "            if len(tgt_content) > 2000:\n",
        "                target_segment = tgt_content[300:1800]\n",
        "            else:\n",
        "                target_segment = tgt_content\n",
        "\n",
        "            tgt_emb = labse_model.encode(target_segment, convert_to_tensor=True)\n",
        "            sem_score = util.pytorch_cos_sim(query_emb, tgt_emb).item()\n",
        "\n",
        "            final_score = (kw_score * 0.3) + (sem_score * 0.7)\n",
        "            final_results.append((final_score, f_path, sem_score, kw_score))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    final_results.sort(key=lambda x: x[0], reverse=True)\n",
        "    return final_results\n",
        "\n",
        "# ==============================================================================\n",
        "# GIAO DIỆN CHƯƠNG TRÌNH (CLI)\n",
        "# ==============================================================================\n",
        "print(\"==============================================\")\n",
        "print(\"CHƯƠNG TRÌNH TÌM KIẾM TÀI LIỆU SONG NGỮ (V5)\")\n",
        "print(\"==============================================\")\n",
        "print(\"[1] Tìm kiếm trong bộ dữ liệu nội bộ\")\n",
        "print(\"[2] Tìm kiếm online (chức năng đang hoàn thiện)\")\n",
        "print(\"==============================================\")\n",
        "\n",
        "mode_choice = input(\"Chọn chế độ (1 hoặc 2): \").strip()\n",
        "\n",
        "if mode_choice == '1':\n",
        "    print(\"\\nDanh sách các file văn bản Hán hiện có:\")\n",
        "    all_han_files = sorted(glob.glob(os.path.join(HAN_FOLDER, \"**/*.txt\"), recursive=True))\n",
        "\n",
        "    if all_han_files:\n",
        "        for i, f in enumerate(all_han_files):\n",
        "            ten_goc = os.path.basename(f)\n",
        "            print(f\"[{i}] {ten_goc}  -->  {dich_ten_file_han(ten_goc)}\")\n",
        "\n",
        "        try:\n",
        "            file_idx = int(input(f\"\\nChọn file cần đối chiếu (0 - {len(all_han_files)-1}): \"))\n",
        "            target_file = all_han_files[file_idx]\n",
        "            print(f\"\\nFile đã chọn: {os.path.basename(target_file)}\")\n",
        "\n",
        "            results = find_best_match_v5(target_file, VIET_FOLDER)\n",
        "\n",
        "            print(\"\\nKết quả xếp hạng cuối cùng:\")\n",
        "            for i, (f_score, fname, sem_s, kw_s) in enumerate(results):\n",
        "                print(f\"Hạng {i+1}: {os.path.basename(fname)}\")\n",
        "                print(f\"  - Điểm tổng hợp: {f_score*100:.2f}%\")\n",
        "                print(f\"  - Ngữ nghĩa: {sem_s*100:.1f}% | Từ khóa: {kw_s*100:.1f}%\")\n",
        "        except Exception as e:\n",
        "            print(f\"Lỗi khi xử lý lựa chọn file: {e}\")\n",
        "    else:\n",
        "        print(\"Không tìm thấy file Hán trong thư mục dữ liệu.\")\n",
        "\n",
        "elif mode_choice == '2':\n",
        "    print(\"Chức năng tìm kiếm online hiện đang trong quá trình phát triển.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7eCXuXSKMibj",
        "outputId": "64ae806b-9b93-43b3-97de-b78a2933eb02"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "CHƯƠNG TRÌNH TÌM KIẾM TÀI LIỆU SONG NGỮ (V5)\n",
            "==============================================\n",
            "[1] Tìm kiếm trong bộ dữ liệu nội bộ\n",
            "[2] Tìm kiếm online (chức năng đang hoàn thiện)\n",
            "==============================================\n",
            "Chọn chế độ (1 hoặc 2): 1\n",
            "\n",
            "Danh sách các file văn bản Hán hiện có:\n",
            "[0] 外紀卷之一.txt  -->  Ngoại Kỷ Quyển 1\n",
            "[1] 外紀卷之三.txt  -->  Ngoại Kỷ Quyển 3\n",
            "[2] 外紀卷之二.txt  -->  Ngoại Kỷ Quyển 2\n",
            "[3] 外紀卷之五.txt  -->  Ngoại Kỷ Quyển 5\n",
            "[4] 外紀卷之四.txt  -->  Ngoại Kỷ Quyển 4\n",
            "[5] 本紀卷之一.txt  -->  Bản Kỷ Quyển 1\n",
            "[6] 本紀卷之七.txt  -->  Bản Kỷ Quyển 7\n",
            "[7] 本紀卷之三.txt  -->  Bản Kỷ Quyển 3\n",
            "[8] 本紀卷之九.txt  -->  Bản Kỷ Quyển 9\n",
            "[9] 本紀卷之二.txt  -->  Bản Kỷ Quyển 2\n",
            "[10] 本紀卷之五.txt  -->  Bản Kỷ Quyển 5\n",
            "[11] 本紀卷之八.txt  -->  Bản Kỷ Quyển 8\n",
            "[12] 本紀卷之六.txt  -->  Bản Kỷ Quyển 6\n",
            "[13] 本紀卷之十.txt  -->  Bản Kỷ Quyển 10\n",
            "[14] 本紀卷之十一.txt  -->  Bản Kỷ Quyển 11\n",
            "[15] 本紀卷之十七.txt  -->  Bản Kỷ Quyển 17\n",
            "[16] 本紀卷之十三.txt  -->  Bản Kỷ Quyển 13\n",
            "[17] 本紀卷之十九.txt  -->  Bản Kỷ Quyển 19\n",
            "[18] 本紀卷之十二.txt  -->  Bản Kỷ Quyển 12\n",
            "[19] 本紀卷之十五.txt  -->  Bản Kỷ Quyển 15\n",
            "[20] 本紀卷之十八.txt  -->  Bản Kỷ Quyển 18\n",
            "[21] 本紀卷之十六.txt  -->  Bản Kỷ Quyển 16\n",
            "[22] 本紀卷之十四.txt  -->  Bản Kỷ Quyển 14\n",
            "[23] 本紀卷之四.txt  -->  Bản Kỷ Quyển 4\n",
            "[24] 續編卷之一.txt  -->  Tục Biên Quyển 1\n",
            "[25] 續編卷之三.txt  -->  Tục Biên Quyển 3\n",
            "[26] 續編卷之二.txt  -->  Tục Biên Quyển 2\n",
            "[27] 續編卷之五.txt  -->  Tục Biên Quyển 5\n",
            "[28] 續編卷之四.txt  -->  Tục Biên Quyển 4\n",
            "\n",
            "Chọn file cần đối chiếu (0 - 28): 10\n",
            "\n",
            "File đã chọn: 本紀卷之五.txt\n",
            "Đang phân tích nội dung văn bản nguồn bằng Gemini...\n",
            "Số từ khóa trích xuất được: 15\n",
            "Đoạn dịch mẫu dùng để so khớp ngữ nghĩa (rút gọn): Năm Bính Tuất, Kiến Trung thứ hai [1228] (năm Bảo Khánh thứ hai đời Tống). Mùa x...\n",
            "\n",
            "Giai đoạn 1: Lọc sơ bộ theo từ khóa\n",
            "Chọn 5 ứng viên vào vòng đánh giá ngữ nghĩa:\n",
            " - Bản Kỷ - Quyển V.txt | Mức khớp từ khóa: 67%\n",
            " - Bản Kỷ - Quyển IV.txt | Mức khớp từ khóa: 47%\n",
            " - Bản Kỷ - Quyển XI.txt | Mức khớp từ khóa: 33%\n",
            " - Bản Kỷ - Quyển X.txt | Mức khớp từ khóa: 27%\n",
            " - Bản Kỷ - Quyển VIII.txt | Mức khớp từ khóa: 20%\n",
            "\n",
            "Giai đoạn 2: So khớp ngữ nghĩa bằng embedding (LaBSE)\n",
            "\n",
            "Kết quả xếp hạng cuối cùng:\n",
            "Hạng 1: Bản Kỷ - Quyển V.txt\n",
            "  - Điểm tổng hợp: 72.08%\n",
            "  - Ngữ nghĩa: 74.4% | Từ khóa: 66.7%\n",
            "Hạng 2: Bản Kỷ - Quyển IV.txt\n",
            "  - Điểm tổng hợp: 68.13%\n",
            "  - Ngữ nghĩa: 77.3% | Từ khóa: 46.7%\n",
            "Hạng 3: Bản Kỷ - Quyển VIII.txt\n",
            "  - Điểm tổng hợp: 63.11%\n",
            "  - Ngữ nghĩa: 81.6% | Từ khóa: 20.0%\n",
            "Hạng 4: Bản Kỷ - Quyển XI.txt\n",
            "  - Điểm tổng hợp: 60.54%\n",
            "  - Ngữ nghĩa: 72.2% | Từ khóa: 33.3%\n",
            "Hạng 5: Bản Kỷ - Quyển X.txt\n",
            "  - Điểm tổng hợp: 54.16%\n",
            "  - Ngữ nghĩa: 65.9% | Từ khóa: 26.7%\n"
          ]
        }
      ]
    }
  ]
}